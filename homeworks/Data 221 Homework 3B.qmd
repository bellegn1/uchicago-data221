---
header-includes:
   - \usepackage{amsmath,amsthm,amsfonts,amssymb,amscd,enumitem,fancyhdr,fullpage,graphicx,hyperref,lastpage,listings,mathrsfs,xcolor}
   - \geometry{top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm}
   - \usepackage[T1]{fontenc}
   - \usepackage[font=small,labelfont=bf,tableposition=top]{caption}
   - \DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}
   - \setlength{\parindent}{0.0in}
   - \setlength{\parskip}{0.05in}
   - \pagestyle{fancyplain}
   - \headheight 35pt
   - \lhead{due Monday, January 30 at 11:59pm}
   - \chead{\textbf{\Large Homework 3B}}
   - \rhead{DATA 221 \\ Winter 2023}
   - \lfoot{}
   - \cfoot{}
   - \rfoot{\small\thepage}
   - \headsep 1.5em
output:
  pdf_document:
    number_section: false
urlcolor: blue
---

<!-- \thispagestyle{fancy} -->
```{r, setup, include=FALSE}
# Install additional packages if needed:
# install.packages('kableExtra')

# Load necessary packages
require(mosaic)
library(kableExtra)
#library(mosaic)   # Load additional packages here 
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",   # slightly smaller font for code
  cache = TRUE)   
```

\section{Overfitting}

This question asks to you produce a graph demonstrating overfitting like that given in Hastie Elements of Statistical Learning Figure 2.4 (pictured below). The underlying source of the points in the graph is a mixture of normal distributions. We will have to 1. Generate random parameters for this distribution, 2. Generate samples from the distribution for training, and 3. Generate another (large) set of samples for testing. The random data sets you generate should look like a shotgun target.

The method for generating the data sets we need is as follows:

\begin{itemize}
  \item Start by generating 10 means for Class A (orange) and 10 means for Class B (blue) from a bivariate normal distribution. The parameter values for each class are given below:
  \begin{itemize}
    \item Class 1: $\mathbf{\mu}_A = (0, 1), \mathbf{\Sigma} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
    \item Class 2: $\mu_B = (1, 0), \mathbf{\Sigma} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
  \end{itemize}  
  \item For the training data, generate 10 data points from a 2-dimensional normal with standard deviation 1/3 for each (of the 20) clusters, a.k.a., $\mathbf{\mu}_{Ak} = (x_{1k}, x_{2k}), \mathbf{\Sigma} = \begin{bmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{bmatrix}$ for data generated in Class A, cluster $k$ and $\mathbf{\mu}_{2k} = (\mu_{1k}, \mu_{2k}), \mathbf{\Sigma} = \begin{bmatrix} \frac{1}{3} & 0 \\ 0 & \frac{1}{3} \end{bmatrix}$ for data generated in Class B.
\end{itemize}  

This is now a distribution in two dimensions with 10 clusters for Class A and 10 clusters for Class B! Be ready to generate 10,000 more samples from these distributions; keep track of the cluster means since you will need them later. You may also want to set a seed to maintain reproducibility.

\begin{enumerate}
  \item Generate 200 points (100 from Class A, 100 from Class B) from the normal-mixture data set as described above. Plot a scatterplot.
  \item Visualize the Bayes decision boundary between the two classes, the surfaces where the (true) density in Class A equals the density in Class B. You can use contour maps to approximate the boundary or you can solve for the boundaries numerically.
\end{enumerate}  

<!-- \includegraphics[width=3in]{src/hastie-bayesian.png} -->

